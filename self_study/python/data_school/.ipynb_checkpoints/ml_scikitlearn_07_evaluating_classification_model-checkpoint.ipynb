{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2eb8c9d",
   "metadata": {},
   "source": [
    "## Evaluating a classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c226b01",
   "metadata": {},
   "source": [
    "* What is the purpose of model evaluation and what are some common evaluation procedures?\n",
    "* What is the usage of classification accuracy, and what are its limitations?\n",
    "* How does a confussion matrix describe the performance of a classifier?\n",
    "* What metrics can be computed from a confusion matrix?\n",
    "* How can you adjust classifer performance by changing the classification threshold?\n",
    "* What is the purpose of an ROC curve?\n",
    "* How does Area Under the Curve (AUC) differ from classification accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d89149",
   "metadata": {},
   "source": [
    "## Review of model evaluation\n",
    "\n",
    "* Need a way to choose between models.\n",
    "* We use a **model evaluation procedure** to estimate how well a model will generalize to out-of-sample data.\n",
    "* This requires a **model evaluation metric** to quantify the model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320ac41a",
   "metadata": {},
   "source": [
    "## Model evaluation metrics\n",
    "\n",
    "* **Regression problems**. Mean Absolute Error, Mean Squared Error, Root Mean Squared Error.\n",
    "* **Classification problems**. Classification Accuracy.\n",
    "\n",
    "However there are many other metrics for both of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "371833d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data into a pandas DataFrame\n",
    "import pandas as pd\n",
    "path = 'https://raw.githubusercontent.com/justmarkham/scikit-learn-videos/master/data/pima-indians-diabetes.data'\n",
    "col_names = ['pregnant', 'glucose', 'bp', 'skin', 'insulin', 'bmi', 'pedigree', 'age', 'label']\n",
    "pima = pd.read_csv(path, header=None, names=col_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163a368b",
   "metadata": {},
   "source": [
    "**Question**. Can we predict the diabetes status of a patient given their health measurements?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ce533b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pregnant</th>\n",
       "      <th>glucose</th>\n",
       "      <th>bp</th>\n",
       "      <th>skin</th>\n",
       "      <th>insulin</th>\n",
       "      <th>bmi</th>\n",
       "      <th>pedigree</th>\n",
       "      <th>age</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pregnant  glucose  bp  skin  insulin   bmi  pedigree  age  label\n",
       "0         6      148  72    35        0  33.6     0.627   50      1\n",
       "1         1       85  66    29        0  26.6     0.351   31      0\n",
       "2         8      183  64     0        0  23.3     0.672   32      1\n",
       "3         1       89  66    23       94  28.1     0.167   21      0\n",
       "4         0      137  40    35      168  43.1     2.288   33      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pima.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49322f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [\"pregnant\", \"insulin\",\"bmi\",\"age\"]\n",
    "X = pima[feature_cols]\n",
    "y = pima[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b45c13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01379669",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b16601d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96eeab8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_class = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add3e170",
   "metadata": {},
   "source": [
    "**Classification accuracy:** percentage of correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "024e4020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6770833333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9607a3",
   "metadata": {},
   "source": [
    "However, every time we train a classification model we should compare the accuracy with **null accuracy** which is the accuracy that could be achieved by always predicting the most frequent class in the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2de2210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3229166666666667"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculating the proportion of ones\n",
    "y_test.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c43ef7a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6770833333333333"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculating the proportion of zeroes\n",
    "1-y_test.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b46e125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6770833333333333"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(y_test.mean(), 1-y_test.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a734c8d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.677083\n",
       "1    0.322917\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()/len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b83ddef",
   "metadata": {},
   "source": [
    "This shows a weakness of classification accuracy. It does not tell anything about the underlying distribution of the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86d2f21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 1 0 0 1 1 0 0 1 1 0 0 0 0 1 0 0 0 1 1 0 0 0]\n",
      "[0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# The model has no problem predicting the zeroes but it is so difficult for it to predict the ones.\n",
    "print(y_test.values[0:25])\n",
    "print(y_pred_class[0:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50996488",
   "metadata": {},
   "source": [
    "## Confusion matrix\n",
    "\n",
    "It is a contingency table that describes the performance of a classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "09f7ca11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[114  16]\n",
      " [ 46  16]]\n"
     ]
    }
   ],
   "source": [
    "cf = metrics.confusion_matrix(y_test, y_pred_class)\n",
    "print(metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3982e6e6",
   "metadata": {},
   "source": [
    "**Important**. All metrics in sklearn expect the true values as its first argument."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0453eb6",
   "metadata": {},
   "source": [
    "**Basic terminology**\n",
    "\n",
    "* Upper left. True negatives (TN).\n",
    "* Lower right. True positives (TP).\n",
    "* Upper right. False positives (FP) (Type I error).\n",
    "* Lower left. False negatives (FN) (Type II error)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a070bc7a",
   "metadata": {},
   "source": [
    "## Metrics computed from a confusion matrix\n",
    "\n",
    "**Classification accuracy**. Overall, how often is the classifier incorrect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "895cf426",
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = cf[1][1]\n",
    "TN = cf[0][0]\n",
    "FP = cf[0][1]\n",
    "FN = cf[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7d5e34cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6770833333333334\n"
     ]
    }
   ],
   "source": [
    "accuracy = (TP+TN)/float(TP+TN+FP+FN)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302094af",
   "metadata": {},
   "source": [
    "**Classification error**. Aka misclassification rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cf76f9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3229166666666667\n"
     ]
    }
   ],
   "source": [
    "class_error = (FP+FN)/float(TP+TN+FP+FN)\n",
    "print(class_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1950c3a0",
   "metadata": {},
   "source": [
    "**Sensitivity**. This answers the question: When the actual value is positive, how often is the prediction correct?\n",
    "\n",
    "AKA **true positive rate** or **recall**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cfbee856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25806451612903225\n"
     ]
    }
   ],
   "source": [
    "sens = TP/float(TP+FN)\n",
    "print(sens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b50366e",
   "metadata": {},
   "source": [
    "**Specificity**. This answers the question: When the actual values is negative, how often the prediction is correct?\n",
    "\n",
    "This measures how specific (or selective) is the classifier in predicting positive instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "529a17e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8769230769230769\n"
     ]
    }
   ],
   "source": [
    "spec = TN/float(TN+FP)\n",
    "print(spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ee4d80",
   "metadata": {},
   "source": [
    "**False positive rate**. When the actual value is negative, how often the prediction is incorrect?\n",
    "\n",
    "It is also 1 - specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "be54e599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12307692307692308\n"
     ]
    }
   ],
   "source": [
    "fpr = FP/float(TN+FP)\n",
    "print(fpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200ca156",
   "metadata": {},
   "source": [
    "**Precision**. When a positive value is predicted, how often is the prediction correct?\n",
    "\n",
    "This measures how precise is the classifier when predicting positive instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "643cdc8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "prec = TP/float(TP+FP)\n",
    "print(prec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52afd4b7",
   "metadata": {},
   "source": [
    "Other scores are Mathews correlation coefficient and f1 score.\n",
    "\n",
    "Which metric to optimize largely depends on the business subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8ec39211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b827fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
