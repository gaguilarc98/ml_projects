####____HYPERPARAMETER TUNING____####
#Hyperparameter tuning in caret
library(tidyverse)
####____HYPERPARAMETER TUNING____####
#Hyperparameter tuning in caret
library(caret)
####____HYPERPARAMETER TUNING____####
#Hyperparameter tuning in caret
install.packages("caret")
library(caret)
library(tidyverse)
df <- iris
install.packages("gbm")
library(gbm)
df_train <- train(Species~.,data=df, method="gbm",trControl=fitCintrol, verbose=F)
fitControl <- trainControl(method="repeatedcv",number=3, repeats=5)
set.seed(42)
df <- iris
df_train <- train(Species~.,data=df, method="gbm",trControl=fitControl, verbose=F)
df_train
hpgrid <- expand.grid(n.trees=c(50,100,150), interaction.depth=c(1,4,6),
shrinkage=0.1, n.minobsinnode=10)
df_train_tune <- train(Species~.,data=df, method="gbm",trcontrol=fitControl,
verbose=F, tuneGrid=hpgrid)
hpgrid <- expand.grid(n.trees=c(50,100,150), interaction.depth=c(1,4,6),
shrinkage=0.1, n.minobsinnode=10)
set.seed(42)
df_train_tune <- train(Species~.,data=df, method="gbm",trcontrol=fitControl,
verbose=F, tuneGrid=hpgrid)
df_train_tune <- train(Species~.,data=df, method="gbm",trcontrol=fitControl,
verbose=F)
fitControl <- trainControl(method="repeatedcv",number=3, repeats=5)
df_train <- train(Species~.,data=df, method="gbm",trControl=fitControl, verbose=F)
hpgrid <- expand.grid(n.trees=c(50,100,150), interaction.depth=c(1,4,6),
shrinkage=0.1, n.minobsinnode=10)
set.seed(42)
df_train_tune <- train(Species~.,data=df, method="gbm",trControl=fitControl,
verbose=F, tuneGrid=hpgrid)
plot(df_train_tune)
plot(df_train_tune, metric="Kappa",plotType="level")
#Inspect the model tuned to look for the columns names an put it into metric
#plotType="line","scatter","level2
library(ggplot2)
ggplot(df_train_tune)
#ADAPTIVE SAMPLING
#Grid and random search ar not so efficient, so use adaptive resampling instead,
#With this technique hp combinations that performed well are resampled around values
#close to combinations that performed better.
df_ada <- trainControl(method="adaptive_cv",number=3, repeats=5,
search="random",adpative=list(min=2, alpha=0.05,
method="gls",complete=F))
source("~/.active-rstudio-document", echo=TRUE)
install.packages("gbm")
install.packages("caret")
#ADAPTIVE SAMPLING
#Grid and random search ar not so efficient, so use adaptive resampling instead,
#With this technique hp combinations that performed well are resampled around values
#close to combinations that performed better.
df_ada <- trainControl(method="adaptive_cv",number=3, repeats=5,
search="random",adaptive=list(min=2, alpha=0.05,
method="gls",complete=F))
#In mlr the tasks can be:
# * RegrTask()
# * ClassifTask()
# * MultilabelTask()
# * CostSensTask()
library(mlr)
#In mlr the tasks can be:
# * RegrTask()
# * ClassifTask()
# * MultilabelTask()
# * CostSensTask()
install.packages("mlr")
library(mlr)
task <- makeClassifTask(data=df,target = "Species")
#Next define the learner
listLearners()
#Fit model
model <- train(lrn, task)
lrn <- makeLearner("classif.h2o.deeplearning",
fix.factors.prediction = T,#If test and train have different categories
predict.type = "prob")
####____HP TUNING with h2o____####
install.packages("h2o")
library(h2o)
lrn <- makeLearner("classif.h2o.deeplearning",
fix.factors.prediction = T,#If test and train have different categories
predict.type = "prob")
lrn <- makeLearner("classif.gbm",
fix.factors.prediction = T,#If test and train have different categories
predict.type = "prob")
#Fit model
model <- train(lrn, task)
# makeParamSet(makeNumericParam(),
#              makeIntegerParam(),
#              makeDiscreteParam(),
#              makeLogicalParam(),
#              makeDiscreteVectorParam())
getParamSet("classif.randomForest")
# makeParamSet(makeNumericParam(),
#              makeIntegerParam(),
#              makeDiscreteParam(),
#              makeLogicalParam(),
#              makeDiscreteVectorParam())
getParamSet("classif.rpart")
#Now define the tuning method
#ctrl_grid <- makeTuneControlGrid()
#or makeTuneControlRandom()
#finally choose resampling strategy
#cv <- makeResampleDesc("RepCV",predict="both",
#                         folds=5*3)
#tuneParams
#tuneParams(learner, task,resampling, par.set,control)
getDefaultMeasure("classif.rpart")
